{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('./Ames_data.csv', header=[0])\n",
    "msk = np.random.rand(len(all_data)) < 0.7\n",
    "\n",
    "all_data[msk].to_csv('train.csv',index=False)\n",
    "all_data[~msk].to_csv('test.csv',index=False)\n",
    "\n",
    "train_data_orig = pd.read_csv('./train.csv', header=[0])\n",
    "test_data_orig = pd.read_csv('./test.csv', header=[0])\n",
    "test_sale=test_data_orig.copy().Sale_Price\n",
    "test_data_orig.drop('Sale_Price', axis=1, inplace=True)\n",
    "test_data_orig.to_csv('test.csv',index=False)\n",
    "# train_data_orig = all_data[msk]\n",
    "# test_data_orig = all_data[~msk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data_orig.copy()\n",
    "test_data=test_data_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "1. Language used Python\n",
    "2. For feature evaluation and engineering I have used https://www.kaggle.com/leeclemmer/exploratory-data-analysis-of-housing-in-ames-iowa\n",
    "3. All application depemdencies has been added to pipfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find categorical and numerical features\n",
    "1. We divide the features into categorical and numerical features based on there types.\n",
    "2. We also drop Longitude and Latitude from our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_columns(dataset):\n",
    "    num=dataset.select_dtypes(include=['int64','float64']).columns.drop(['PID','Sale_Price','Latitude','Longitude'])\n",
    "    cat = dataset.select_dtypes(include=['object'])\n",
    "    return list(num), list(cat) \n",
    "\n",
    "numerical_features,categorical_features=get_features_columns(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales Price data analysis\n",
    "From the distribution plot for Sales price we see the it is slightly positively skewed. We will do a log transformation for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "# sns.distplot(all_data.Sale_Price)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical feature data analysis\n",
    "1. From the plots below we see that several of the numerical features are positively skewed and we will be log transforming these features\n",
    "2. Couple of features looks like categorical, for example MS_SubClass. We will stringify them and treat as categorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = pd.melt(train_data, value_vars=sorted(numerical_features))\n",
    "# g = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\n",
    "# g = g.map(sns.distplot, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_numerical(dataset):\n",
    "    dataset['MS_SubClass'] = dataset.MS_SubClass.apply(lambda x: str(x))\n",
    "    dataset['Mo_Sold'] = dataset.Mo_Sold.apply(lambda x: str(x))\n",
    "    dataset['Year_Sold'] = dataset.Year_Sold.apply(lambda x: str(x))\n",
    "    return dataset\n",
    "\n",
    "train_data=stringify_numerical(train_data)\n",
    "test_data=stringify_numerical(test_data)\n",
    "\n",
    "numerical_features,categorical_features=get_features_columns(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical data analysis\n",
    "1. From plots below we see that several categorical features can be easily converted to numerical features by assigning some kind of rank to various categories. For example Bsmnt_Cond can be categorised as -\n",
    "    a Poor-0\n",
    "    b Fair -1\n",
    "    c. Typlical-2\n",
    "    d. Good-3\n",
    "    e. Excellent-4\n",
    "2. For Year built we will create ranges for the year and assign it ranking. The older the year built the lower the ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = pd.melt(all_data, value_vars=sorted(categorical_features))\n",
    "# g = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\n",
    "# plt.xticks(rotation='vertical')\n",
    "# g = g.map(sns.countplot, 'value')\n",
    "# [plt.setp(ax.get_xticklabels(), rotation=60) for ax in g.axes.flat]\n",
    "# g.fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numerical_from_categorical(dataset):\n",
    "    dataset.Alley.replace({'No_Alley_Access':0,'Gravel':1, 'Paved':2}, inplace=True)\n",
    "    dataset.Alley= dataset.Alley.apply(lambda i:0 if i not in [0,1,2] else i)\n",
    "    dataset.BsmtFin_Type_1.replace({'No_Basement':0,'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}, inplace=True)\n",
    "    dataset.BsmtFin_Type_1= dataset.BsmtFin_Type_1.apply(lambda i:0 if i not in [0,1,2,3,4,5,6] else i)\n",
    "    dataset.BsmtFin_Type_2.replace({'No_Basement':0,'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}, inplace=True)\n",
    "    dataset.BsmtFin_Type_2= dataset.BsmtFin_Type_2.apply(lambda i:0 if i not in [0,1,2,3,4,5,6] else i)\n",
    "    \n",
    "#     dataset.Bsmt_Cond.replace({'No_Basement':0,'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n",
    "#     dataset.Bsmt_Cond= dataset.Bsmt_Cond.apply(lambda i:0 if i not in [0,1,2,3,4,5] else i)\n",
    "    \n",
    "    dataset.Bsmt_Cond.replace({'No_Basement':0,'Poor':1, 'Fair':2, 'Typical':3, 'Good':4, 'Excellent':5}, inplace=True)\n",
    "    dataset.Bsmt_Cond= dataset.Bsmt_Qual.apply(lambda i:0 if i not in [0,1,2,3,4,5] else i)\n",
    "    \n",
    "    dataset.Bsmt_Exposure.replace({'No_Basement':0,'No':1, 'Mn':2, 'Av':3, 'Gd':4}, inplace=True)\n",
    "    dataset.Bsmt_Exposure= dataset.Bsmt_Exposure.apply(lambda i:0 if i not in [0,1,2,3,4] else i)\n",
    "    \n",
    "    dataset.Bsmt_Qual.replace({'No_Basement':0,'Poor':1, 'Fair':2, 'Typical':3, 'Good':4, 'Excellent':5}, inplace=True)\n",
    "    dataset.Bsmt_Qual= dataset.Bsmt_Qual.apply(lambda i:0 if i not in [0,1,2,3,4,5] else i)\n",
    "    \n",
    "    dataset.Central_Air.replace({'Y':1,'N':0}, inplace=True)\n",
    "    \n",
    "    dataset.Exter_Cond.replace({'Poor':0, 'Fair':1, 'Typical':2, 'Good':3, 'Excellent':4}, inplace=True)\n",
    "    dataset.Exter_Cond= dataset.Exter_Cond.apply(lambda i:0 if i not in [0,1,2,3,4] else i)\n",
    "    \n",
    "    dataset.Exter_Qual.replace({'Fair':0, 'Typical':1, 'Good':2, 'Excellent':3}, inplace=True)\n",
    "    dataset.Exter_Qual= dataset.Exter_Qual.apply(lambda i:0 if i not in [0,1,2,3] else i)\n",
    "    \n",
    "    dataset.Fireplace_Qu.replace({'No_Fireplace':0,'Poor':1, 'Fair':2, 'Typical':3, 'Good':4, 'Excellent':5}, inplace=True)\n",
    "    dataset.Fireplace_Qu= dataset.Fireplace_Qu.apply(lambda i:0 if i not in [0,1,2,3,4,5] else i)\n",
    "    \n",
    "    dataset.Functional.replace({'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8}, inplace=True)\n",
    "    dataset.Functional= dataset.Functional.apply(lambda i:0 if i not in [0,1,2,3,4,5,6,7,8] else i)\n",
    "    \n",
    "    dataset.Garage_Qual.replace({'No_Garage':0,'Poor':1, 'Fair':2, 'Typical':3, 'Good':4, 'Excellent':5}, inplace=True)\n",
    "    dataset.Garage_Qual= dataset.Garage_Qual.apply(lambda i:0 if i not in [0,1,2,3,4,5] else i)\n",
    "    \n",
    "    dataset.Garage_Finish.replace({'No_Garage':0,'Unf':1, 'RFn':2, 'Fin':3}, inplace=True)\n",
    "    dataset.Garage_Finish= dataset.Garage_Finish.apply(lambda i:0 if i not in [0,1,2,3] else i)\n",
    "    \n",
    "    dataset.Garage_Cond.replace({'No_Garage':0,'Poor':1, 'Fair':2, 'Typical':3, 'Good':4, 'Excellent':5}, inplace=True)\n",
    "    dataset.Garage_Cond= dataset.Garage_Cond.apply(lambda i:0 if i not in [0,1,2,3,4,5] else i)\n",
    "    \n",
    "    dataset.Heating_QC.replace({'Poor':0, 'Fair':1, 'Typical':2, 'Good':3, 'Excellent':4}, inplace=True)\n",
    "    dataset.Heating_QC= dataset.Heating_QC.apply(lambda i:0 if i not in [0,1,2,3,4] else i)\n",
    "    \n",
    "    dataset.Kitchen_Qual.replace({'Poor':0, 'Fair':1, 'Typical':2, 'Good':3, 'Excellent':4}, inplace=True)\n",
    "    dataset.Kitchen_Qual= dataset.Kitchen_Qual.apply(lambda i:0 if i not in [0,1,2,3,4] else i)\n",
    "    \n",
    "    dataset.Lot_Shape.replace({'Regular':3, 'Slightly_Irregular':2, 'Moderately_Irregular':1, 'Irregular':0}, inplace=True)\n",
    "    dataset.Lot_Shape= dataset.Lot_Shape.apply(lambda i:0 if i not in [0,1,2,3] else i)\n",
    "    \n",
    "    dataset.Land_Slope.replace({'Sev':0, 'Mod':1, 'Gtl':2}, inplace=True)\n",
    "    dataset.Land_Slope= dataset.Land_Slope.apply(lambda i:0 if i not in [0,1,2] else i)\n",
    "    \n",
    "    dataset.Land_Contour.replace({'Low':0, 'HLS':1, 'Bnk':2, 'Lvl':3}, inplace=True)\n",
    "    dataset.Land_Contour= dataset.Land_Contour.apply(lambda i:0 if i not in [0,1,2,3] else i)\n",
    "    \n",
    "    dataset.Utilities.replace({'NoSeWa':0, 'NoSewr':1, 'AllPub':2}, inplace=True)\n",
    "    dataset.Utilities= dataset.Utilities.apply(lambda i:0 if i not in [0,1,2] else i)\n",
    "    \n",
    "    dataset.Overall_Cond.replace({'Very_Poor':0, 'Poor':1,'Below_Average':2, 'Average':3, 'Above_Average':4,'Fair':5, 'Good':6, 'Very_Good':7,'Excellent':8}, inplace=True)\n",
    "    dataset.Overall_Cond= dataset.Overall_Cond.apply(lambda i:0 if i not in [0,1,2,3,4,5,6,7,8] else i)\n",
    "    \n",
    "    dataset.Overall_Qual.replace({'Very_Poor':0, 'Poor':1,'Below_Average':2, 'Average':3, 'Above_Average':4,'Fair':5, 'Good':6, 'Very_Good':7,'Excellent':8}, inplace=True)\n",
    "    dataset.Overall_Qual= dataset.Overall_Qual.apply(lambda i:0 if i not in [0,1,2,3,4,5,6,7,8] else i)\n",
    "    \n",
    "    dataset.Paved_Drive.replace({'No_Pavement':0, 'Dirt_Gravel':1, 'Paved':2}, inplace=True)\n",
    "    dataset.Paved_Drive= dataset.Paved_Drive.apply(lambda i:0 if i not in [0,1,2] else i)\n",
    "    \n",
    "    dataset.Pool_QC.replace({'No_Pool':0, 'Fair':1, 'Typical':2, 'Good':3,'Excellent':4}, inplace=True)\n",
    "    dataset.Pool_QC= dataset.Pool_QC.apply(lambda i:0 if i not in [0,1,2,3,4] else i)\n",
    "    \n",
    "    dataset.Street.replace({'Pave':1, 'Grvl':0}, inplace=True)\n",
    "    dataset.Street= dataset.Street.apply(lambda i:0 if i not in [0,1] else i)\n",
    "    \n",
    "    def year_fixer(yr):\n",
    "        if yr<1950:\n",
    "            return 0\n",
    "        if yr>=1950 and yr<1960:\n",
    "            return 1\n",
    "        if yr>=1960 and yr<1970:\n",
    "            return 2\n",
    "        if yr>=1970 and yr<1980:\n",
    "            return 3\n",
    "        if yr>=1980 and yr<1990:\n",
    "            return 4\n",
    "        if yr>=1990 and yr<2000:\n",
    "            return 5\n",
    "        if yr>=2000 and yr<2010:\n",
    "            return 6\n",
    "        if yr>=2010 :\n",
    "            return 7\n",
    "        \n",
    "        \n",
    "    dataset.Year_Built = dataset.Year_Built.apply(year_fixer)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "train_data=create_numerical_from_categorical(train_data)\n",
    "test_data=create_numerical_from_categorical(test_data)\n",
    "    \n",
    "numerical_features,categorical_features=get_features_columns(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_and_fix_features_and_label(dataset):\n",
    "    all_data_copy = dataset.copy()\n",
    "\n",
    "    all_data_copy.drop('PID', axis=1, inplace=True)\n",
    "    all_data_copy.drop('Longitude', axis=1, inplace=True)\n",
    "    all_data_copy.drop('Latitude', axis=1, inplace=True)\n",
    "    all_response=None\n",
    "    if 'Sale_Price' in all_data_copy:\n",
    "        all_response = all_data_copy.Sale_Price\n",
    "        all_data_copy.drop('Sale_Price', axis=1, inplace=True)\n",
    "\n",
    "    return all_data_copy,all_response\n",
    "\n",
    "train_feature_data,train_Y =split_and_fix_features_and_label(train_data)\n",
    "test_feature_data,test_Y1 =split_and_fix_features_and_label(test_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing Null data\n",
    "From the plot below we see that Garage_Yr_Blt has null data. We will mark null values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_data = train_feature_data.isnull().sum() / train_feature_data.shape[0]\n",
    "# missing_data[missing_data > 0].\\\n",
    "#     sort_values(ascending=True).\\\n",
    "#     plot(kind='barh', figsize=(10,6))\n",
    "# plt.title('Percentage of missing values')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(featureset):\n",
    "    featureset.Garage_Yr_Blt.fillna(0, inplace=True)\n",
    "    return featureset\n",
    "\n",
    "train_feature_data=fill_na(train_feature_data)\n",
    "test_feature_data=fill_na(test_feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_skewed_data(featureset,label):\n",
    "    features = numerical_features\n",
    "    featureset.loc[:,features] = np.log1p(featureset.loc[:,features])\n",
    "    label= np.log1p(label)\n",
    "    return featureset,label\n",
    "    \n",
    "train_feature_data,train_Y=log_skewed_data(train_feature_data,train_Y)\n",
    "# test_feature_data,test_Y=log_skewed_data(test_feature_data,test_Y)\n",
    "test_feature_data.loc[:,numerical_features] = np.log1p(test_feature_data.loc[:,numerical_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical data transformation\n",
    "We will convert all categorical data by one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotrod_featureset(featureset):\n",
    "    return pd.get_dummies(featureset)\n",
    "    \n",
    "train_feature_data=onehotrod_featureset(train_feature_data)\n",
    "test_feature_data=onehotrod_featureset(test_feature_data)\n",
    "\n",
    "train_feature_data, test_feature_data = train_feature_data.align(test_feature_data, join='inner', axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def root_mean_sqr_error(actual_data,prediction_data):\n",
    "#    return np.sqrt(np.mean(np.power((np.log(prediction_data)-np.log(actual_data)),2)))\n",
    "    return np.sqrt(mean_squared_error(np.log(actual_data), prediction_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize numerical features\n",
    "Before creating the model we will center numerical data to mean 0 and scale it to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# train_feature_data.loc[:,numerical_features]  = scaler.fit_transform(train_feature_data.loc[:,numerical_features] )\n",
    "# test_feature_data.loc[:,numerical_features] = scaler.fit_transform(test_feature_data.loc[:,numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.DataFrame()\n",
    "output['PID'] =test_data_orig.PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_predict(model,outputfile,params):\n",
    "    clf = GridSearchCV(model,params, verbose=1,n_jobs=10,cv=3)\n",
    "\n",
    "    model = clf.fit(train_feature_data,train_Y)\n",
    "\n",
    " \n",
    "    train_pred = model.predict(train_feature_data)\n",
    "    rmse_train=root_mean_sqr_error(train_Y,train_pred)\n",
    "    print (f'RMSE for training data is :{rmse_train}')\n",
    "\n",
    "    test_pred = model.predict(test_feature_data)\n",
    "    rmse_test=root_mean_sqr_error(test_sale,test_pred)\n",
    "    \n",
    "#     print(test_y)\n",
    "#     print(test_y -np.expm1(test_pred))\n",
    "#     print(test_pred)\n",
    "#     print(test_pred)\n",
    "\n",
    "    output['Sale_Price'] = np.expm1(test_pred)\n",
    "    output.to_csv(outputfile,index=False)\n",
    "    print (f'RMSE for testing data is :{rmse_test}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for prediction\n",
    "For prediction I have used two models from XGBoost package. For the first model I have used a range of values mainly for different sampling rates to find best sampling rate for a conservative model. Also I have used a depth of 8, which may actually result in overfitting\n",
    "For second model I have used parameters which are conservative. Also I have used a range of lower depths so as to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   9 out of   9 | elapsed:   12.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training data is :9.546114366060301\n",
      "RMSE for testing data is :0.12726218123546013\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "# xgb_model=xgb_predict(xgb_model,'mysubmission1.txt',{'min_child_weight':[i/10.0 for i in range(0,10)], 'gamma':[i/10.0 for i in range(0,10)],'subsample':[i/10.0 for i in range(5,10)],'colsample_bytree':[i/10.0 for i in range(5,10)], 'max_depth': [8]})\n",
    "\n",
    "\n",
    "xgb_model2 = xgb.XGBRegressor()\n",
    "xgb_model2=xgb_predict(xgb_model2,'mysubmission2.txt',{'colsample_bytree':[0.2],'gamma':[0.0],'learning_rate':[0.01],'max_depth':[2,4,6],'min_child_weight':[1.5],'n_estimators':[7200],'reg_alpha':[0.9],'reg_lambda':[0.6],'subsample':[0.2]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
